{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    # %pip install einops\n",
    "\n",
    "    # Code to make sure output widgets display\n",
    "    from google.colab import output\n",
    "\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "    !wget -q https://github.com/EffiSciencesResearch/ML4G-2.0/archive/refs/heads/master.zip\n",
    "    !unzip -o /content/master.zip 'ML4G-2.0-master/workshops/optimisation/*'\n",
    "    !mv --no-clobber ML4G-2.0-master/workshops/optimisation/* .\n",
    "    !rm -r ML4G-2.0-master\n",
    "\n",
    "    print(\"Imports & installations complete!\")\n",
    "\n",
    "else:\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/optimisation/optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Optimization\n",
    "\n",
    "Today's material focuses on understanding the standard methods of optimisation in machine learning. You're going to learn about the training loop and different optimizers.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "In the preparation work, you have seen how backpropagation works. Today, we're going to use the gradients produced by backpropagation for optimizing a loss function using gradient descent.\n",
    "\n",
    "A loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n",
    "\n",
    "We actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\lambda$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n",
    "\n",
    "$$ \\theta_t \\leftarrow \\theta_{t-1} - \\lambda \\nabla L(\\theta_{t-1}) $$\n",
    "\n",
    "We know that an infinitesimal step will decrease the loss, but a finite step will only do so if the loss function is linear enough in the neighbourhood of the current parameters. If the loss function is too curved, we might actually increase our loss.\n",
    "\n",
    "The biggest advantage of this algorithm is that for N bytes of parameters, you only need N additional bytes of memory to store the gradients, which are of the same shape as the parameters. GPU memory is very limited, so this is an extremely relevant consideration. The amount of computation needed is also minimal: one multiply and one add per parameter.\n",
    "\n",
    "The biggest disadvantage is that we're completely ignoring the curvature of the loss function, not captured by the gradient consisting of partial derivatives. Intuitively, we can take a larger step if the loss function is flat in some direction or a smaller step if it is very curved. Generally, you could represent this by some matrix P that pre-multiplies the gradients to rescale them to account for the curvature. P is called a preconditioner, and gradient descent is equivalent to approximating P by an identity matrix, which is a very bad approximation.\n",
    "\n",
    "Most competing optimizers can be interpreted as trying to do something more sensible for P, subject to the constraint that GPU memory is at a premium. In particular, constructing P explicitly is infeasible, since it's an $N \\times N$ matrix and N can be hundreds of billions. One idea is to use a diagonal P, which only requires N additional memory. An example of a more sophisticated scheme is [Shampoo](https://arxiv.org/pdf/1802.09568.pdf).\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Why is it called Shampoo?</summary>\n",
    "\n",
    "You put shampoo on your hair before using conditioner, and this method is a pre-conditioner. :D\n",
    "\n",
    "</details>\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The learning rate is an example of a **hyperparameter**, which will be described below. As a reminder, a regular parameter is an adjustable value with the special and extremely convenient property that we can differentiate the loss with respect to the parameter, allowing us to efficiently learn good values for the parameter using gradient descent. In other words, the process of training is a function that takes a dataset, a model architecture, and a random seed and outputs model parameters.\n",
    "\n",
    "The learning rate, in contrast, cannot be determined by this scheme. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters using a dataset of pairs of $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each iteration (a single pair) requires running the inner training loop.\n",
    "\n",
    "Due to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n",
    "\n",
    "In addition, a hyperparameter isn't necessarily a single continuous value like the learning rate. Discrete unordered choices such as padding type as well as discrete ordered choices such as the number of layers in the network or the width of each convolution are all common. You will also need to choose between functions for optimizers, nonlinearities, or learning rate scheduling, of which there are an infinite number of possibilities, requiring us to select a small subset to test.\n",
    "\n",
    "More broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1801.01548.pdf) demonstrated that it's possible to find architectures superior to human-designed ones.\n",
    "\n",
    "In the second part of today's material, you will learn about various strategies for searching over hyperparameters.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "The terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n",
    "\n",
    "- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n",
    "- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n",
    "- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n",
    "\n",
    "The class `torch.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course.\n",
    "\n",
    "## Batch Size\n",
    "\n",
    "In addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n",
    "\n",
    "At a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n",
    "\n",
    "For a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n",
    "\n",
    "You will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n",
    "\n",
    "Powers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n",
    "\n",
    "## Computing Gradients in PyTorch\n",
    "\n",
    "Recall that gradients are only saved for `Tensor`s for which `requires_grad=True`. For convenience, `nn.Parameter` automatically sets `requires_grad=True` on the wrapped `Tensor`. As you call `torch` functions, PyTorch tracks the relevant information needed in case you call `backward` later on, at which point it does the actual computation to compute the gradient and stores it in the `Tensor`'s `grad` field.\n",
    "\n",
    "Also recall that PyTorch accumulates gradients across multiple `backward` calls. So if your tensor's `grad` already contains a value, after calling `backward` again it will have the sum of the original value and the new gradient. This behavior comes in handy in many situations, such as computing gradients over multiple runs on a GPU as part of a single batch. Suppose you choose a batch size of 32, but only 8 inputs fit on your GPU. A typical loss function for a batch computes the sum of losses over each example, so you can compute the losses 8 at a time and sum their gradients, producing the same result as running all 32 inputs at once.\n",
    "\n",
    "### Stopping gradients with `torch.no_grad` or `torch.inference_mode`\n",
    "\n",
    "You may not want PyTorch to track gradients for some computations despite involving tensors with `requires_grad=True`. In this case, you can wrap the computation in the `with torch.inference_mode()` context to prevent this tracking. Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Union, Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.figure\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import optim_tests\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "if MAIN:\n",
    "    x = t.ones(1, 2, 3, requires_grad=True)\n",
    "    y = x * x\n",
    "    with t.inference_mode():\n",
    "        z = x * x\n",
    "    print(f\"y requires grad: {y.requires_grad}; z requires grad: {z.requires_grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Result: `y requires grad: True; z requires grad: False`\n",
    "\n",
    "## Common Themes in Gradient-Based Optimizers\n",
    "\n",
    "### Weight Decay\n",
    "\n",
    "Weight decay means that on each iteration, in addition to a regular step, we also shrink each parameter very slightly towards 0 by multiplying a scaling factor close to 1, e.g. 0.9999. Empirically, this seems to help but there are no proofs that apply to deep neural networks (if you know of one, let me know!).\n",
    "\n",
    "In the case of linear regression, weight decay is mathematically equivalent to having a prior that each parameter is Gaussian distributed - in other words it's very unlikely that the true parameter values are very positive or very negative. This is an example of \"inductive bias\" - we make an assumption that helps us in the case where it's justified, and hurts us in the case where it's not justified.\n",
    "\n",
    "For a `Linear` layer, it's common practice to apply weight decay only to the weight and not the bias. It's also common to not apply weight decay to the parameters of a batch normalization layer. Again, there is empirical evidence (such as [Jai et al 2018](https://arxiv.org/pdf/1807.11205.pdf)) and there are heuristic arguments to justify these choices, but no rigorous proofs.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Momentum means that the step includes a term proportional to a moving average of past gradients. [Distill.pub](https://distill.pub/2017/momentum/) has a great article on momentum.\n",
    "\n",
    "### Many More Optimizer Variants\n",
    "\n",
    "[Sebastian Ruder's blog](https://ruder.io/optimizing-gradient-descent/) goes into detail on many more variants of gradient descent.\n",
    "\n",
    "## Exercise: Learning To Reproduce a Picture\n",
    "\n",
    "In this exercise you will train a neural network to memorize a picture of your choice! Your network will implement a function from the $(x, y)$ coordinates of a pixel to three numbers $(R, G, B)$ representing the color of that pixel. Implement the `ImageMemorizer` network with three Linear layers and two ReLUs (generally, you don't want a ReLU after the last Linear layer). Test that your model matches the reference.\n",
    "\n",
    "#### Note: The \"device\" variable\n",
    "\n",
    "A useful idiom when writing code to run on both CPU and GPU is to declare a `device` variable at the top of your notebook and then use it later on when creating or moving tensors. We've done this for you today.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement the `train_one_epoch` function below.\n",
    "\n",
    "- Use the `to()` method of a `Tensor` to send the data to the device indicated by the global variable `device`.\n",
    "- You can convert a one-element tensor to a regular Python number using the `item` method.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>It's not working and I'm confused!</summary>\n",
    "\n",
    "- Did you remember to call `optimizer.zero_grad()` before each forward pass?\n",
    "- Does `model.parameters()` return what you expect?\n",
    "- Are you calling `backward()` on the mean loss over the batch items? Note that if you don't use the mean, the magnitude of the gradients scales up linearly with the batch size, which is not what you want.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Optimization With Rosenbrock's Banana\n",
    "\n",
    "\"Rosenbrock's Banana\" is a (relatively) famous function that has a simple equation but is challenging to optimize because of the shape of the loss landscape.\n",
    "\n",
    "Use `plot_rosenbrock` to plot the log of the function. Where is the minimum?\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The first term is minimized when $x=a$ and the second term is minimized when $y = x^2 = a^2$. For $a=1$, it's $(1, 1)$. Looking at the plot, this seems reasonable.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "\n",
    "def plot_rosenbrock(\n",
    "    xmin=-2, xmax=2, ymin=-1, ymax=3, n_points=50, log_scale=False\n",
    ") -> matplotlib.figure.Figure:\n",
    "    \"\"\"Plot the rosenbrocks_banana function over the specified domain.\n",
    "\n",
    "    If log_scale is True, take the logarithm of the output before plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = t.linspace(xmin, xmax, n_points)\n",
    "    y = t.linspace(ymin, ymax, n_points)\n",
    "    xx = repeat(x, \"x -> y x\", y=n_points)\n",
    "    yy = repeat(y, \"y -> y x\", x=n_points)\n",
    "    zs = rosenbrocks_banana(xx, yy)\n",
    "    contour = ax.contourf(x, y, t.log(zs) if log_scale else zs)\n",
    "    ax.contour(contour)\n",
    "    ax.set(xlabel=\"x\", ylabel=\"y\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "plot_rosenbrock()\n",
    "plot_rosenbrock(log_scale=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Optimize The Banana\n",
    "\n",
    "Implement the `opt_banana` function using `torch.optim.SGD`. Starting from `(-1.5, 2.5)`, run your function and add the resulting trajectory of `(x, y)` pairs to your contour plot. Did it find the minimum? Play with the learning rate and momentum a bit and see how close you can get within 100 iterations.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>I'm not sure if my `opt_banana` is implemented properly.</summary>\n",
    "\n",
    "With a learning rate of `0.001` and momentum of `0.98`, my SGD was able to reach `[ 1.0234299 ,  1.198282 ]` after 100 iterations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>I'm getting \"Can't call numpy() on Tensor that requires grad\" and I don't know why!</summary>\n",
    "\n",
    "This is a protective mechanism built into PyTorch. The idea is that once you convert your `Tensor` to NumPy, PyTorch can no longer track gradients, but you might not understand this and expect backprop to work on NumPy arrays.\n",
    "\n",
    "All you need to do to convince PyTorch you're a responsible adult is to call `detach()` on the tensor first, which returns a view that does not require grad and isn't part of the computation graph.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_banana(xy: t.Tensor, n_iters: int, lr=0.001, momentum=0.98):\n",
    "    \"\"\"Optimize the banana starting from the specified point.\n",
    "\n",
    "    xy: shape (2,). The (x, y) starting point.\n",
    "    n_iters: number of steps.\n",
    "\n",
    "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
    "    \"\"\"\n",
    "    assert xy.requires_grad\n",
    "\n",
    "    # Remember the 4 steps of the basic training loop\n",
    "    ...\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
    "    xys = opt_banana(xy, n_iters=100).numpy()\n",
    "    fig = plot_rosenbrock(log_scale=True)\n",
    "    fig.axes[0].plot(xys[:, 0], xys[:, 1], color=\"r\", linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```py\n",
    "def opt_banana(xy: t.Tensor, n_iters: int, lr=0.001, momentum=0.98):\n",
    "    \"\"\"Optimize the banana starting from the specified point.\n",
    "\n",
    "    xy: shape (2,). The (x, y) starting point.\n",
    "    n_iters: number of steps.\n",
    "\n",
    "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
    "    \"\"\"\n",
    "    assert xy.requires_grad\n",
    "\n",
    "    out = xy.new_zeros((n_iters, 2))\n",
    "    optimizer = t.optim.SGD([xy], lr=lr, momentum=momentum)\n",
    "\n",
    "    for i in tqdm(range(n_iters)):\n",
    "        out[i] = xy.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss = rosenbrocks_banana(xy[0], xy[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return out\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Build Your Own Optimizers\n",
    "\n",
    "Now let's build our own drop-in replacement for these three classes from `torch.optim`. The documentation pages for these algorithms have pseudocode you can use to implement your step method.\n",
    "\n",
    "First Read this article:\n",
    "https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\n",
    "\n",
    "### Gotcha: In-Place Operations\n",
    "\n",
    "Be careful with expressions like `x = x + y` and `x += y`. They are NOT equivalent in Python.\n",
    "\n",
    "- The first one allocates a new `Tensor` of the appropriate size and adds `x` and `y` to it, then rebinds `x` to point to the new variable. The original `x` is not modified.\n",
    "- The second one modifies the storage referred to by `x` to contain the sum of `x` and `y` - it is an \"in-place\" operation.\n",
    "  - Another way to write the in-place operation is `x.add_(y)` (the trailing underscore indicates an in-place operation).\n",
    "  - A third way to write the in-place operation is `torch.add(x, y, out=x)`.\n",
    "- This is rather subtle, so make sure you are clear on the difference. This isn't specific to PyTorch; the built-in Python `list` follows similar behavior: `x = x + y` allocates a new list, while `x += y` is equivalent to `x.extend(y)`.\n",
    "- In general, the first version calls the method `x.__add__(y)` while the second calls `x.__iadd__(y)`, and these two methods can have arbitrary semantics.\n",
    "\n",
    "The tricky thing that happens here is that both the optimizer and the `Module` in your model have a reference to the same `Parameter` instance. Do we want to use in-place operations in our optimizer?\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Solution - In-place Operations</summary>\n",
    "\n",
    "You MUST use in-place operations in your optimizer because we want the model to see the change to the Parameter's storage on the next forward pass. If your optimizer allocates a new tensor, the model won't know anything about the new tensor and will continue to use the old, unmodified version.\n",
    "\n",
    "</details>\n",
    "\n",
    "### More Tips\n",
    "\n",
    "- The provided `params` might be a generator, in which case you can only iterate over it once before the generator is exhausted. Copy it into a `list` to be able to iterate over it repeatedly.\n",
    "- Your step function shouldn't modify the gradients. Use the `with torch.inference_mode():` context for this. Fun fact: you can instead use `@torch.inference_mode()` (note the preceding `@`) as a method decorator to do the same thing.\n",
    "- If you create any new tensors, they should be on the same device as the corresponding parameter. Use `torch.zeros_like()` or similar for this.\n",
    "- Be careful not to mix up `Parameter` and `Tensor` types in this step.\n",
    "- The actual PyTorch implementations have an additional feature called parameter groups where you can specify different hyperparameters for each group of parameters. You can ignore this for today.\n",
    "\n",
    "Tip: It is possible to not use if conditions in sgd and Adam. This simplifies the code.\n",
    "\n",
    "### SGD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why do we have to zero the gradient in PyTorch? \n",
    "- Why do we use the word 'stochastic' in 'Stochastic gradient descent' in the context of deep learning?\n",
    "\n",
    "SGD:\n",
    "- Below, read the method zero_grad. You can note that in PyTorch, to zero a gradient means assigning None.\n",
    "- We will implement successively the pure SGD, the sgd with weight decay, and the SGD with momentum.\n",
    "\n",
    "Implement steps:\n",
    "1. `# update` Implement the most basic version of SGD possible\n",
    "- Why do we need the 'with torch.inference_mode()' context manager?\n",
    "2. `# weight_decay`: What is the formula of the update when there is some weight_decay (ie when we penalize each parameter squared)? Assume wd absorbs the constant.\n",
    "- Tip: let's say we optimize `L(X, y) = (ax_1 + bx_2 + c - y)^2` with respect to `a`, `b` and `c`.\n",
    "- Adding weight_decay means that instead of minimizing `L`, we minimize `g(X, y) =  L(X, y) + wd(a^2 + b^2 + c^2)/2`\n",
    "- For this example, what is the formula of the gradient wrt `a`,`b` and `c`?\n",
    "- In the code, at the beginning of the step function, replace the gradient by `g = p.grad + self.wd * p`\n",
    "3. `# momentum`: Why do we need `self.running_average` in the `__init__`? Add momentum. Separate the cases `self.momentum` equals zero or not.\n",
    "\n",
    "There are multiple ways to implement SGD, so don't panic if there is some ambiguity and look at the solution to compare with the PyTorch implementation when you have used every variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[t.nn.parameter.Parameter],\n",
    "        lr: float,\n",
    "        momentum: float,\n",
    "        weight_decay: float,\n",
    "    ):\n",
    "        \"\"\"Implements SGD with momentum.\n",
    "\n",
    "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
    "\n",
    "        \"\"\"\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.wd = weight_decay\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # running average of the gradient if there is some momentum\n",
    "        self.running_average = [None for _ in self.params]\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def step(self) -> None:\n",
    "        with t.inference_mode():\n",
    "            for i, p in enumerate(self.params):\n",
    "                # weight decay\n",
    "                ...\n",
    "\n",
    "                # momentum\n",
    "                ...\n",
    "\n",
    "                # update\n",
    "                ...\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    optim_tests.test_sgd(SGD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Adam\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam, by far the most used optimizer.\n",
    "\n",
    "It's a combination of SGD+RMSProps and uses one momentum for the gradient, and another for the gradient squared.\n",
    "\n",
    "1. Adam\n",
    "Tip: There is no `if` condition in Adam because `beta1` and `beta2` are always stricly positive.\n",
    "- `sum_of_gradient = previous_sum_of_gradient * beta1 + gradient * (1 - beta1)` (SGD+Momentum)\n",
    "- `sum_of_gradient_squared = previous_sum_of_gradient_squared * beta2 + gradient² * (1- beta2)` (RMSProp)\n",
    "- `delta = -learning_rate * sum_of_gradient / sqrt(sum_of_gradient_squared)`\n",
    "- Update the gradient\n",
    "\n",
    "2. More stability + regularization\n",
    "- Add `self.eps` to the denominator, outside the square root.\n",
    "- At the beginning of the step function, replace the gradient by `g = p.grad + self.wd * p`\n",
    "\n",
    "3. Adam + Correction\n",
    "- In the `__init__`, `self.b1` and `self.b2` are a list of zeros, so we need a correction:\n",
    "- In the update, use a correction: `b1_hat = self.b1[i] / (1.0 - self.beta1**self.t)`\n",
    "- Generally `beta1 = 0.9`, and `beta2 = 0.999`.\n",
    "- Same for `b2_hat`.\n",
    "\n",
    "\n",
    "NB: The correction is important only at the beginning of the optimization process (self.t small).\n",
    "The the correction is needed because self.b1 and self.b2 are initialized at zero.\n",
    "\n",
    "Try to pass the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[t.nn.parameter.Parameter],\n",
    "        lr: float = 0.001,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-08,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"Implements Adam.\n",
    "\n",
    "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
    "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "        \"\"\"\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas  # momenti of b1 and b2\n",
    "        self.eps = eps\n",
    "        self.wd = weight_decay\n",
    "\n",
    "        # sum_of_gradient for each param\n",
    "        # & sum_of_gradient_squared for each param\n",
    "        self.b1 = [torch.zeros_like(p) for p in self.params]\n",
    "        self.b2 = [torch.zeros_like(p) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def zero_grad(self) -> None: ...\n",
    "\n",
    "    def step(self) -> None: ...\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    optim_tests.test_adam(Adam)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
